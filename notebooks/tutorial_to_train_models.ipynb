{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ae74f4",
   "metadata": {},
   "source": [
    "# Gaia Synthetic Light Curve Classification Tutorial\n",
    "\n",
    "Welcome to the Gaia synthetic light curve classification tutorial! This notebook will guide you through the full workflow for preparing, transforming, and classifying synthetic Gaia light curve data. You will learn how to:\n",
    "\n",
    "- Download and organize synthetic light curve data\n",
    "- Sample and split data for training and validation\n",
    "- Transform light curves into polar hexbin images\n",
    "- Prepare datasets for both system type and spot detection tasks\n",
    "- Train and evaluate deep learning models (ResNet, ViT) using PyTorch\n",
    "\n",
    "Follow the cells below for step-by-step instructions and examples. All code is modular and importable for easy reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bac6f",
   "metadata": {},
   "source": [
    "First step: creating training and validation datasets. We have light curves stored on our cloud. We need to download them on a local machine.\n",
    "\n",
    "**Download Link:** [Gaia synthetic light curve data](https://u.pcloud.link/publink/show?code=kZMm285Zoy7Q3IAQOakIshhv4jTeH8OAtS4y#folder=25637440779)\n",
    "\n",
    "Download the following zip files into `data` folder:\n",
    "\n",
    "*   `synthetic_gaia.zip`\n",
    "\n",
    "Unzip it inside the `data` folder.\n",
    "\n",
    "In the new `synthetic_gaia` folder there are:\n",
    "*   `detached_nospot_gaia.csv`\n",
    "*   `detached_spot_gaia.csv`\n",
    "*   `overcontact_nospot_gaia.csv`\n",
    "*   `overcontact_spot_gaia.csv`\n",
    "\n",
    "These are synthesised data, and we will create training and validation samples from them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec59eb8",
   "metadata": {},
   "source": [
    "### 1. Create directories for train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ../data/gaia_dataset/train\n",
      "Created directory: ../data/gaia_dataset/val\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_root = \"../data/gaia_dataset\"\n",
    "for split in [\"train\", \"val\"]:\n",
    "    split_dir = os.path.join(output_root, split)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    print(f\"Created directory: {split_dir}\")\n",
    "# Now you have ../data/gaia_dataset/train and ../data/gaia_dataset/val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6a6b7",
   "metadata": {},
   "source": [
    "### 2. Select train and validation samples for each class\n",
    "You do not have to unzip the files, we will work them as it is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef214a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 1000 train and 100 val samples for detached nospot.\n",
      "Selected 1000 train and 100 val samples for detached spot.\n",
      "Selected 1000 train and 100 val samples for detached spot.\n",
      "Selected 1000 train and 100 val samples for overcontact nospot.\n",
      "Selected 1000 train and 100 val samples for overcontact nospot.\n",
      "Selected 1000 train and 100 val samples for overcontact spot.\n",
      "Selected 1000 train and 100 val samples for overcontact spot.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define your zip files and class mapping\n",
    "csv_files = {\n",
    "    \"detached_nospot_gaia.csv\": (\"detached\", \"nospot\"),\n",
    "    \"detached_spot_gaia.csv\": (\"detached\", \"spot\"),\n",
    "    \"overcontact_nospot_gaia.csv\": (\"overcontact\", \"nospot\"),\n",
    "    \"overcontact_spot_gaia.csv\": (\"overcontact\", \"spot\"),\n",
    "}\n",
    "data_dir = \"../data/gaia_synthetic_data\"\n",
    "sample_size = 1000  # Set the number of samples per group (adjust as needed)\n",
    "indexes = {}\n",
    "val_fraction = 0.1  # 10% for validation\n",
    "\n",
    "for csv_name, (system_type, spot_type) in csv_files.items():\n",
    "    csv_path = os.path.join(data_dir, csv_name)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    n_val = int(sample_size * val_fraction)\n",
    "    n_train = sample_size\n",
    "    # Randomly sample without replacement from all available indices\n",
    "    all_idx = np.random.choice(df.index, n_train + n_val, replace=False)\n",
    "    val_idx = np.random.choice(all_idx, n_val, replace=False)\n",
    "    train_idx = np.setdiff1d(all_idx, val_idx)\n",
    "    indexes[(system_type, spot_type)] = {\n",
    "        'train': train_idx.tolist(),\n",
    "        'val': val_idx.tolist()\n",
    "    }\n",
    "    print(f\"Selected {len(train_idx)} train and {len(val_idx)} val samples for {system_type} {spot_type}.\")\n",
    "# Now 'indexes' contains all the train/val indices for each class, ready for further use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385d4b99",
   "metadata": {},
   "source": [
    "### 3. Generate Polar Hexbin Images from Light Curve DataFrames\n",
    "This section provides functions to generate polar hexbin images from light curve DataFrames, with noise, outlier, and random point removal settings based on the passband ('gaia', 'tess', 'ogle')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ecd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')  # Add the scripts directory to the Python path\n",
    "from make_polar_hexbin_images import create_polar_hexbin, create_images_from_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9de49",
   "metadata": {},
   "source": [
    "### 4. Generate and save polar hexbin images for all training and validation samples \n",
    "This cell will use the previously defined sample indexes and the imported functions to generate images for each class (detached, overcontact) and split, saving them in the correct output folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge spot/nospot for each system type, then generate and save images for all splits and classes\n",
    "from collections import defaultdict\n",
    "\n",
    "data_dir = \"../data/gaia_synthetic_data\"\n",
    "output_root = \"../data/gaia_dataset\"\n",
    "passband = \"gaia\"  # Change if using other passbands\n",
    "\n",
    "# Merge indexes for detached and overcontact (combine spot/nospot)\n",
    "merged_indexes = defaultdict(lambda: {'train': [], 'val': []})\n",
    "for (system_type, spot_type), idx_dict in indexes.items():\n",
    "    merged_indexes[system_type]['train'].extend(idx_dict['train'])\n",
    "    merged_indexes[system_type]['val'].extend(idx_dict['val'])\n",
    "\n",
    "for system_type in [\"detached\", \"overcontact\"]:\n",
    "    # Combine both spot and nospot dataframes for each system type\n",
    "    dfs = []\n",
    "    for spot_type in [\"nospot\", \"spot\"]:\n",
    "        zip_name = f\"{system_type}_{spot_type}_gaia.zip\"\n",
    "        zip_path = os.path.join(data_dir, zip_name)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            csv_name = [f for f in z.namelist() if f.endswith(\".csv\")][0]\n",
    "            with z.open(csv_name) as f:\n",
    "                df = pd.read_csv(f)\n",
    "            dfs.append(df)\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        idxs = merged_indexes[system_type][split]\n",
    "        df_split = df_all.loc[idxs]\n",
    "        out_dir = os.path.join(output_root, split, system_type)\n",
    "        create_images_from_dataframe(df_split, out_dir, n_start=0, passband=passband)\n",
    "        print(f\"Saved {len(df_split)} images to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f234796",
   "metadata": {},
   "source": [
    "## 5. Train ResNet50 Model\n",
    "\n",
    "Now, when we have prepared data, we could start training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f1f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wera/Max_astro/Slovakia/EBML/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/wera/Max_astro/Slovakia/EBML/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wera/Max_astro/Slovakia/EBML/data/gaia_dataset\n",
      "Resolved data_dir: /Users/wera/Max_astro/Slovakia/EBML/data/gaia_dataset\n",
      "Checking for folder: /Users/wera/Max_astro/Slovakia/EBML/data/gaia_dataset/train (exists: True)\n",
      "Checking for folder: /Users/wera/Max_astro/Slovakia/EBML/data/gaia_dataset/val (exists: True)\n",
      "Subfolders in /Users/wera/Max_astro/Slovakia/EBML/data/gaia_dataset/train: ['overcontact', 'detached']\n",
      "Subfolders in /Users/wera/Max_astro/Slovakia/EBML/data/gaia_dataset/val: ['overcontact', 'detached']\n",
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [14:36<00:00,  7.01s/it]\n",
      "100%|██████████| 125/125 [14:36<00:00,  7.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1533 Acc: 0.9407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:51<00:00,  3.97s/it]\n",
      "100%|██████████| 13/13 [00:51<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1210 Acc: 0.9425\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [15:17<00:00,  7.34s/it]\n",
      "100%|██████████| 125/125 [15:17<00:00,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0977 Acc: 0.9653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:46<00:00,  3.55s/it]\n",
      "100%|██████████| 13/13 [00:46<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0637 Acc: 0.9750\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [12:42<00:00,  6.10s/it]\n",
      "100%|██████████| 125/125 [12:42<00:00,  6.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1079 Acc: 0.9627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:44<00:00,  3.39s/it]\n",
      "100%|██████████| 13/13 [00:44<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.3611 Acc: 0.8925\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [14:23<00:00,  6.91s/it]\n",
      "100%|██████████| 125/125 [14:23<00:00,  6.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0854 Acc: 0.9712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:45<00:00,  3.51s/it]\n",
      "100%|██████████| 13/13 [00:45<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0589 Acc: 0.9775\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [17:01<00:00,  8.17s/it]\n",
      "100%|██████████| 125/125 [17:01<00:00,  8.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0810 Acc: 0.9710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:59<00:00,  4.54s/it]\n",
      "100%|██████████| 13/13 [00:59<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1109 Acc: 0.9650\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [20:20<00:00,  9.77s/it]\n",
      "100%|██████████| 125/125 [20:20<00:00,  9.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0808 Acc: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:59<00:00,  4.58s/it]\n",
      "100%|██████████| 13/13 [00:59<00:00,  4.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0651 Acc: 0.9800\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [19:35<00:00,  9.40s/it]\n",
      "100%|██████████| 125/125 [19:35<00:00,  9.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0775 Acc: 0.9718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:45<00:00,  3.52s/it]\n",
      "100%|██████████| 13/13 [00:45<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0788 Acc: 0.9700\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [12:38<00:00,  6.07s/it]\n",
      "100%|██████████| 125/125 [12:38<00:00,  6.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0525 Acc: 0.9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:46<00:00,  3.54s/it]\n",
      "100%|██████████| 13/13 [00:46<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0693 Acc: 0.9750\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [14:23<00:00,  6.91s/it]\n",
      "100%|██████████| 125/125 [14:23<00:00,  6.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0367 Acc: 0.9882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:52<00:00,  4.07s/it]\n",
      "100%|██████████| 13/13 [00:52<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0523 Acc: 0.9775\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [16:19<00:00,  7.83s/it]\n",
      "100%|██████████| 125/125 [16:19<00:00,  7.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0351 Acc: 0.9850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:52<00:00,  4.01s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0645 Acc: 0.9750\n",
      "Validation Accuracy: 0.9725\n",
      "Validation Accuracy: 0.9725\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "import model_pytorch_ResNet\n",
    "\n",
    "data_dir = \"../data/gaia_dataset\"\n",
    "import os\n",
    "print(os.path.abspath(data_dir))\n",
    "\n",
    "dataloaders = model_pytorch_ResNet.prepare_training(data_dir, batch_size=32)\n",
    "\n",
    "trained_model = model_pytorch_ResNet.train_model(\n",
    "    model_pytorch_ResNet.resnet,\n",
    "    dataloaders,\n",
    "    model_pytorch_ResNet.criterion,\n",
    "    model_pytorch_ResNet.optimizer,\n",
    "    model_pytorch_ResNet.scheduler,\n",
    "    model_pytorch_ResNet.num_epochs\n",
    ")\n",
    "\n",
    "import torch\n",
    "torch.save(trained_model.state_dict(), \"../models/model_ResNet.pth\")\n",
    "model_pytorch_ResNet.evaluate_model(trained_model, dataloaders['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430a46d",
   "metadata": {},
   "source": [
    "## 6. Train ViT Model \n",
    "You can use the next code cell to train ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01692f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wera/Max_astro/Slovakia/EBML/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wera/Max_astro/Slovakia/EBML/data/gaia_dataset\n",
      "Using device: cpu\n",
      "Loading images from: ../data/gaia_dataset/train\n",
      "Loaded 4000 images for train\n",
      "Loading images from: ../data/gaia_dataset/val\n",
      "Loaded 400 images for val\n",
      "Epoch 1/5\n",
      "----------\n",
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train phase: 100%|██████████| 125/125 [21:29<00:00, 10.32s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5653 Acc: 0.7113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val phase: 100%|██████████| 13/13 [00:55<00:00,  4.28s/it]\n",
      "val phase: 100%|██████████| 13/13 [00:55<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1674 Acc: 0.9450\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train phase: 100%|██████████| 125/125 [18:58<00:00,  9.11s/it]\n",
      "train phase: 100%|██████████| 125/125 [18:58<00:00,  9.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1563 Acc: 0.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val phase: 100%|██████████| 13/13 [00:53<00:00,  4.14s/it]\n",
      "val phase: 100%|██████████| 13/13 [00:53<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1122 Acc: 0.9525\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train phase: 100%|██████████| 125/125 [24:24<00:00, 11.72s/it]\n",
      "train phase: 100%|██████████| 125/125 [24:24<00:00, 11.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0933 Acc: 0.9655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val phase: 100%|██████████| 13/13 [01:24<00:00,  6.47s/it]\n",
      "val phase: 100%|██████████| 13/13 [01:24<00:00,  6.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0523 Acc: 0.9850\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train phase: 100%|██████████| 125/125 [26:09<00:00, 12.56s/it]\n",
      "train phase: 100%|██████████| 125/125 [26:09<00:00, 12.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0689 Acc: 0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val phase: 100%|██████████| 13/13 [01:29<00:00,  6.91s/it]\n",
      "val phase: 100%|██████████| 13/13 [01:29<00:00,  6.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0860 Acc: 0.9600\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train phase: 100%|██████████| 125/125 [24:58<00:00, 11.99s/it]\n",
      "train phase: 100%|██████████| 125/125 [24:58<00:00, 11.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0533 Acc: 0.9808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val phase: 100%|██████████| 13/13 [00:54<00:00,  4.16s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.0674 Acc: 0.9800\n",
      "Validation Accuracy: 0.9800\n",
      "Validation Accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "import model_pytorch_ViT\n",
    "\n",
    "data_dir = \"../data/gaia_dataset\"\n",
    "import os\n",
    "print(os.path.abspath(data_dir))\n",
    "\n",
    "dataloaders = model_pytorch_ViT.prepare_training(data_dir, batch_size=32)\n",
    "\n",
    "model_vit = model_pytorch_ViT.create_vit_model(num_classes=2)\n",
    "criterion, optimizer, scheduler = model_pytorch_ViT.get_loss_optimizer_scheduler(model_vit)\n",
    "\n",
    "trained_model = model_pytorch_ViT.train_model(\n",
    "    model_vit,\n",
    "    dataloaders,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=5\n",
    ")\n",
    "\n",
    "import torch\n",
    "torch.save(trained_model.state_dict(), \"../models/model_ViT.pth\")\n",
    "model_pytorch_ViT.evaluate_model(trained_model, dataloaders['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8563f",
   "metadata": {},
   "source": [
    "## 7. Spot Detection Dataset Preparation (Detached Systems)\n",
    "\n",
    "In this section, we will prepare a dataset specifically for spot detection in detached binary systems. We will:\n",
    "- Use the previously sampled indexes for detached systems with and without spots.\n",
    "- Generate polar hexbin images for each sample using the provided transformation function.\n",
    "- Save the images into the new directory structure: `../data/gaia_spot_dataset/train/{spot,nospot}` and `../data/gaia_spot_dataset/val/{spot,nospot}`.\n",
    "\n",
    "This dataset will allow you to train a classifier to distinguish between detached systems with and without spots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eef3c5",
   "metadata": {},
   "source": [
    "### 8. Create directories for spot detection dataset (detached systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f75086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ../data/gaia_spot_dataset/train/spot\n",
      "Created directory: ../data/gaia_spot_dataset/train/nospot\n",
      "Created directory: ../data/gaia_spot_dataset/val/spot\n",
      "Created directory: ../data/gaia_spot_dataset/val/nospot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "spot_dataset_root = \"../data/gaia_spot_dataset\"\n",
    "for split in [\"train\", \"val\"]:\n",
    "    for spot_class in [\"spot\", \"nospot\"]:\n",
    "        split_dir = os.path.join(spot_dataset_root, split, spot_class)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        print(f\"Created directory: {split_dir}\")\n",
    "# Now you have ../data/gaia_spot_dataset/train/{spot,nospot} and ../data/gaia_spot_dataset/val/{spot,nospot}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ea0ce",
   "metadata": {},
   "source": [
    "### 9. Generate and save images for spot detection (detached systems only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e44f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 images to ../data/gaia_spot_dataset/train/nospot\n",
      "Saved 1000 images to ../data/gaia_spot_dataset/train/spot\n",
      "Saved 1000 images to ../data/gaia_spot_dataset/train/spot\n",
      "Saved 100 images to ../data/gaia_spot_dataset/val/nospot\n",
      "Saved 100 images to ../data/gaia_spot_dataset/val/nospot\n",
      "Saved 100 images to ../data/gaia_spot_dataset/val/spot\n",
      "Saved 100 images to ../data/gaia_spot_dataset/val/spot\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Use the same indexes dictionary as before\n",
    "spot_dataset_root = \"../data/gaia_spot_dataset\"\n",
    "data_dir = \"../data/gaia_synthetic_data\"\n",
    "passband = \"gaia\"\n",
    "\n",
    "# Load detached spot and nospot dataframes\n",
    "dfs = {}\n",
    "for spot_type in [\"nospot\", \"spot\"]:\n",
    "    zip_name = f\"detached_{spot_type}_gaia.zip\"\n",
    "    zip_path = os.path.join(data_dir, zip_name)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        csv_name = [f for f in z.namelist() if f.endswith(\".csv\")][0]\n",
    "        with z.open(csv_name) as f:\n",
    "            df = pd.read_csv(f)\n",
    "        dfs[spot_type] = df\n",
    "\n",
    "# For each split, generate images for spot/nospot\n",
    "for split in [\"train\", \"val\"]:\n",
    "    for spot_type in [\"nospot\", \"spot\"]:\n",
    "        idxs = indexes[(\"detached\", spot_type)][split]\n",
    "        df_split = dfs[spot_type].loc[idxs]\n",
    "        out_dir = os.path.join(spot_dataset_root, split, spot_type)\n",
    "        create_images_from_dataframe(df_split, out_dir, n_start=0, passband=passband)\n",
    "        print(f\"Saved {len(df_split)} images to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d4c93",
   "metadata": {},
   "source": [
    "Now you can use Train ResNet model or Train ViT model code cells.\n",
    "Just change 'data_dir' and the name of .pth file where do you want to save your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0ff07",
   "metadata": {},
   "source": [
    "Preparing the necessary dataset in a way that showed in this tutorial one can train models for different passbands presented in our dataset in the cloud."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
